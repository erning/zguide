<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Testing and Simulation</title><meta name="generator" content="DocBook XSL Stylesheets V1.76.1" /><link rel="home" href="index.html" title="The ZeroMQ Guide - for C Developers" /><link rel="up" href="ch08.html" title="Chapter 8. A Framework for Distributed Computing" /><link rel="prev" href="ch08s06.html" title="Group Messaging" /><link rel="next" href="ch08s08.html" title="Distributed Logging and Monitoring" /></head><body><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">Testing and Simulation</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="ch08s06.html">Prev</a> </td><th width="60%" align="center">Chapter 8. A Framework for Distributed Computing</th><td width="20%" align="right"> <a accesskey="n" href="ch08s08.html">Next</a></td></tr></table><hr /></div><div class="sect1" title="Testing and Simulation"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="idp21368784"></a>Testing and Simulation</h2></div></div></div><p>When you build a product out of pieces, and this includes a distributed framework like Zyre, the only way to know that it will work properly in real life is to simulate real activity on each piece.</p><div class="sect2" title="On Assertions"><div class="titlepage"><div><div><h3 class="title"><a id="idp21369536"></a>On Assertions</h3></div></div></div><p>The proper use of assertions is one of the hallmarks of a professional programmer.</p><p>Our confirmation bias as creators makes it hard to test our work properly. We tend to write tests to prove the code works, rather than trying to prove it doesn't. There are many reasons for this. We pretend to ourselves and others that we can be (could be) perfect, when in fact we consistently make mistakes. Bugs in code are seen as "bad", rather than "inevitable", so psychologically we want to see fewer of them, not uncover more of them. "He writes perfect code" is a compliment rather than a euphemism for "he never takes risks so his code is as boring and heavily used as cold spaghetti".</p><p>Some cultures teach us to aspire to perfection and punish mistakes in education and work, which makes this attitude worse. To accept that we're fallible, and then to learn how to turn that into profit rather than shame is one of the hardest intellectual exercises in any profession. We leverage our fallibilities by working with others and by challenging our own work sooner, not later.</p><p>One trick that makes it easier is to use assertions. Assertions are not a form of error handling. They are executable theories of fact. The code asserts, "At this point, such and such must be true" and if the assertion fails, the code kills itself.</p><p>The faster you can prove code incorrect, the faster and more accurately you can fix it. Believing that code works and proving that it behaves as expected is less science, more magical thinking. It's far better to be able to say, "<code class="literal">libzmq</code> has five hundred assertions and despite all my efforts, not one of them fails".</p><p>So the Zyre code base is scattered with assertions, and particularly a couple on the code that deals with the state of peers. This is the hardest aspect to get right: peers need to track each other and exchange state accurately or things stop working. The algorithms depend on asynchronous messages flying around and I'm pretty sure the initial design has flaws. It always does.</p><p>And as I test the original Zyre code by starting and stopping instances of <code class="literal">zre_ping</code> by hand, every so often I get an assertion failure. Running by hand doesn't reproduce these often enough, so let's make a proper tester tool.</p></div><div class="sect2" title="On Up-Front Testing"><div class="titlepage"><div><div><h3 class="title"><a id="idp21374256"></a>On Up-Front Testing</h3></div></div></div><p>Being able to fully test the real behavior of individual components in the laboratory can make a 10x or 100x difference to the cost of your project. That confirmation bias engineers have to their own work makes up-front testing incredibly profitable, and late-stage testing incredibly expensive.</p><p>I'll tell you a short story about a project we worked on in the late 1990's. We provided the software and other teams provided the hardware for a factory automation project. Three or four teams brought their experts on-site, which was a remote factory (funny how the polluting factories are always in remote border country).</p><p>One of these teams, a firm specializing in industrial automation, built ticket machines: kiosks, and software to run on them. Nothing unusual: swipe a badge, choose an option, receive a ticket. They assembled two of these kiosks on-site, each week bringing some more bits and pieces. Ticket printers, monitor screens, special keypads from Israel. The stuff had to be resistant against dust because the kiosks sat outside. Nothing worked. The screens were unreadable in the sun. The ticket printers continually jammed and misprinted. The internals of the kiosk just sat on wooden shelving. The kiosk software crashed regularly. It was comedic except that the project really, <span class="emphasis"><em>really</em></span> had to work and so we spent weeks and then months on-site helping the other teams debug their bits and pieces until it worked.</p><p>A year later, there was a second factory, and the same story. By this time the client, was getting impatient. So when they came to the third and largest factory, a year later, we jumped up and said, "please let us make the kiosks and the software and everything".</p><p>We made a detailed design for the software and hardware and found suppliers for all the pieces. It took us three months to search the Internet for each component (in those days, the Internet was a lot slower), and another two months to get them assembled into stainless-steel bricks each weighing about twenty kilos. These bricks were two feet square and eight inches deep, with a large flat-screen panel behind unbreakable glass, and two connectors: one for power, one for Ethernet. You loaded up the paper bin with enough for six months, then screwed the brick into a housing, and it automatically booted, found its DNS server, loaded its Linux OS and then application software. It connected to the real server, and showed the main menu. You got access to the configuration screens by swiping a special badge and then entering a code.</p><p>The software was portable so we could test that as we wrote it, and as we collected the pieces from our suppliers we kept one of each so we had a disassembled kiosk to play with. When we got our finished kiosks, they all worked immediately. We shipped them to the client, who plugged them into their housing, switched them on, and went to business. We spent a week or so on-site, and in ten years, one kiosk broke (the screen died, and was replaced).</p><p>Lesson is, test upfront so that when you plug the thing in, you know precisely how it's going to behave. If you haven't tested it upfront, you're going to be spending weeks and months in the field ironing out problems that should never have been there.</p></div><div class="sect2" title="The Zyre Tester"><div class="titlepage"><div><div><h3 class="title"><a id="idp21379512"></a>The Zyre Tester</h3></div></div></div><p>During manual testing, I did hit an assertion rarely. It then disappeared. Because I don't believe in magic, I know that meant the code was still wrong somewhere. So, the next step was heavy-duty testing of the Zyre v0.2.0 code to try to break its assertions, and get a good idea of how it will behave in the field.</p><p>We packaged the discovery and messaging functionality as an <span class="emphasis"><em>interface</em></span> object that the main program creates, works with, and then destroys. We don't use any global variables. This makes it easy to start large numbers of interfaces and simulate real activity, all within one process. And if there's one thing we've learned from writing lots of examples, it's that ØMQ's ability to orchestrate multiple threads in a single process is <span class="emphasis"><em>much</em></span> easier to work with than multiple processes.</p><p>The first version of the tester consists of a main thread that starts and stops a set of child threads, each running one interface, each with a ROUTER, DEALER, and UDP socket (R, D, and U in the diagram)<a class="xref" href="ch08s07.html#figure-68" title="Figure 8.2. Zyre Tester Tool">Figure 8.2, “Zyre Tester Tool”</a>.</p><div class="figure"><a id="figure-68"></a><p class="title"><strong>Figure 8.2. Zyre Tester Tool</strong></p><div class="figure-contents"><div class="mediaobject"><img src="images/fig68.png" alt="Zyre Tester Tool" /></div></div></div><br class="figure-break" /><p>The nice thing is that when I am connected to a WiFi access point, all Zyre traffic (even between two interfaces in the same process) goes across the AP. This means I can fully stress test any WiFi infrastructure with just a couple of PCs running in a room. It's hard to emphasize how valuable this is: if we had built Zyre as, say, a dedicated service for Android, we'd literally need dozens of Android tablets or phones to do any large-scale testing. Kiosks, and all that.</p><p>The focus is now on breaking the current code, trying to prove it wrong. There's <span class="emphasis"><em>no point</em></span> at this stage in testing how well it runs, how fast it is, how much memory it uses, or anything else. We'll work up to trying (and failing) to break each individual functionality, but first, we try to break some of the core assertions I've put into the code.</p><p>These are:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>The first command that any node receives from a peer MUST be <code class="literal">HELLO</code>. In other words, messages <span class="emphasis"><em>cannot</em></span> be lost during the peer-to-peer connection process.</p></li><li class="listitem"><p>The state each node calculates for its peers matches the state each peer calculates for itself. In other words, again, no messages are lost in the network.</p></li><li class="listitem"><p>When my application sends a message to a peer, we have a connection to that peer. In other words, the application only "sees" a peer after we have established a ØMQ connection to it.</p></li></ul></div><p>With ØMQ, there are several cases where we may lose messages. One is the "late joiner" syndrome. Two is when we close sockets without sending everything. Three is when we overflow the high-water mark on a ROUTER or PUB socket. Four is when we use an unknown address with a ROUTER socket.</p><p>Now, I <span class="emphasis"><em>think</em></span> Harmony gets around all these potential cases. But we're also adding UDP to the mix. So the first version of the tester simulates an unstable and dynamic network, where nodes come and go randomly. It's here that things will break.</p><p>Here is the main thread of the tester, which manages a pool of 100 threads, starting and stopping each one randomly. Every ~750 msecs it either starts or stops one random thread. We randomize the timing so that threads aren't all synchronized. After a few minutes, we have an average of 50 threads happily chatting to each other like Korean teenagers in the Gangnam subway station:</p><pre class="programlisting">
int main (int argc, char *argv [])
{
    //  Initialize context for talking to tasks
    zctx_t *ctx = zctx_new ();
    zctx_set_linger (ctx, 100);

    //  Get number of interfaces to simulate, default 100
    int max_interface = 100;
    int nbr_interfaces = 0;
    if (argc &gt; 1)
        max_interface = atoi (argv [1]);

    //  We address interfaces as an array of pipes
    void **pipes = zmalloc (sizeof (void *) * max_interface);

    //  We will randomly start and stop interface threads
    while (!zctx_interrupted) {
        uint index = randof (max_interface);
        //  Toggle interface thread
        if (pipes [index]) {
            zstr_send (pipes [index], "STOP");
            zsocket_destroy (ctx, pipes [index]);
            pipes [index] = NULL;
            zclock_log ("I: Stopped interface (%d running)",
                --nbr_interfaces);
        }
        else {
            pipes [index] = zthread_fork (ctx, interface_task, NULL);
            zclock_log ("I: Started interface (%d running)",
                ++nbr_interfaces);
        }
        //  Sleep ~750 msecs randomly so we smooth out activity
        zclock_sleep (randof (500) + 500);
    }
    zctx_destroy (&amp;ctx);
    return 0;
}
</pre><p>Note that we maintain a <span class="emphasis"><em>pipe</em></span> to each child thread (CZMQ creates the pipe automatically when we use the <code class="literal">zthread_fork</code> method). It's via this pipe that we tell child threads to stop when it's time for them to leave. The child threads do the following (I'm switching to pseudo-code for clarity):</p><pre class="screen">create an interface
while true:
    poll on pipe to parent, and on interface
    if parent sent us a message:
        break
    if interface sent us a message:
        if message is ENTER:
            send a WHISPER to the new peer
        if message is EXIT:
            send a WHISPER to the departed peer
        if message is WHISPER:
            send back a WHISPER 1/2 of the time
        if message is SHOUT:
            send back a WHISPER 1/3 of the time
            send back a SHOUT 1/3 of the time
    once per second:
        join or leave one of 10 random groups
destroy interface
</pre></div><div class="sect2" title="Test Results"><div class="titlepage"><div><div><h3 class="title"><a id="idp21394528"></a>Test Results</h3></div></div></div><p>Yes, we broke the code. Several times, in fact. This was satisfying. I'll work through the different things we found.</p><p>Getting nodes to agree on consistent group status was the most difficult. Every node needs to track the group membership of the whole network, as I already explained in the section "Group Messaging". Group messaging is a pub-sub pattern. <code class="literal">JOIN</code>s and <code class="literal">LEAVE</code>s are analogous to subscribe and unsubscribe messages. It's essential that none of these ever get lost, or we'll find nodes dropping randomly off groups.</p><p>So each node counts the total number of <code class="literal">JOIN</code>s and <code class="literal">LEAVE</code>s it's ever done, and broadcasts this status (as 1-byte rolling counter) in its UDP beacon. Other nodes pick up the status, compare it to their own calculations, and if there's a difference, the code asserts.</p><p>The first problem was that UDP beacons get delayed randomly, so they're useless for carrying the status. When a beacons arrives late, the status is inaccurate and we get a <span class="emphasis"><em>false negative</em></span>. To fix this, we moved the status information into the <code class="literal">JOIN</code> and <code class="literal">LEAVE</code> commands. We also added it to the <code class="literal">HELLO</code> command. The logic then becomes:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Get initial status for a peer from its <code class="literal">HELLO</code> command.</p></li><li class="listitem"><p>When getting a <code class="literal">JOIN</code> or <code class="literal">LEAVE</code> from a peer, increment the status counter.</p></li><li class="listitem"><p>Check that the new status counter matches the value in the <code class="literal">JOIN</code> or <code class="literal">LEAVE</code> command</p></li><li class="listitem"><p>If it doesn't, assert.</p></li></ul></div><p>Next problem we got was that messages were arriving unexpectedly on new connections. The Harmony pattern connects, then sends <code class="literal">HELLO</code> as the first command. This means the receiving peer should always get <code class="literal">HELLO</code> as the first command from a new peer. We were seeing <code class="literal">PING</code>, <code class="literal">JOIN</code>, and other commands arriving.</p><p>This turned out to be due to CZMQ's ephemeral port logic. An ephemeral port is just a dynamically assigned port that a service can get rather than asking for a fixed port number. A POSIX system usually assigns ephemeral ports in the range 0xC000 to 0xFFFF. CZMQ's logic is to look for a free port in this range, bind to that, and return the port number to the caller.</p><p>This sounds fine, until you get one node stopping and another node starting close together, and the new node getting the port number of the old node. Remember that ØMQ tries to re-establish a broken connection. So when the first node stopped, its peers would retry to connect. When the new node appears on that same port, suddenly all the peers connect to it and start chatting like they're old buddies.</p><p>It's a general problem that affects any larger-scale dynamic ØMQ application. There are a number of plausible answers. One is to not reuse ephemeral ports, which is easier said than done when you have multiple processes on one system. Another solution would be to select a random port each time, which at least reduces the risk of hitting a just-freed port. This brings the risk of a garbage connection down to perhaps 1/1000 but it's still there. Perhaps the best solution is to accept that this can happen, understand the causes, and deal with it on the application level.</p><p>We have a stateful protocol that always starts with a <code class="literal">HELLO</code> command. We know that it's possible for peers to connect to us, thinking we're an existing node that went away and came back, and send us other commands. Step one is when we discover a new peer, to destroy any existing peer connected to the same endpoint. It's not a full answer but at least it's polite. Step two is to ignore anything coming in from a new peer until that peer says <code class="literal">HELLO</code>.</p><p>This doesn't require any change to the protocol, but it must be specified in the protocol when we come to it: due to the way ØMQ connections work, it's possible to receive unexpected commands from a <span class="emphasis"><em>well-behaving</em></span> peer and there is no way to return an error code or otherwise tell that peer to reset its connection. Thus, a peer must discard any command from a peer until it receives <code class="literal">HELLO</code>.</p><p>In fact, if you draw this on a piece of paper and think it through, you'll see that you never get a <code class="literal">HELLO</code> from such a connection. The peer will send <code class="literal">PING</code>s and <code class="literal">JOIN</code>s and <code class="literal">LEAVE</code>s and then eventually time out and close, as it fails to get any heartbeats back from us.</p><p>You'll also see that there's no risk of confusion, no way for commands from two peers to get mixed into a single stream on our DEALER socket.</p><p>When you are satisfied that this works, we're ready to move on. This version is tagged in the repository as v0.3.0 and you can <a class="ulink" href="https://github.com/zeromq/zyre/tags" target="_top">download the tarball</a> if you want to check what the code looked like at this stage.</p><p>Note that doing heavy simulation of lots of nodes will probably cause your process to run out of file handles, giving an assertion failure in <code class="literal">libzmq</code>. I raised the per-process limit to 30,000 by running (on my Linux box):</p><pre class="screen">ulimit -n 30000
</pre></div><div class="sect2" title="Tracing Activity"><div class="titlepage"><div><div><h3 class="title"><a id="idp21413160"></a>Tracing Activity</h3></div></div></div><p>To debug the kinds of problems we saw here, we need extensive logging. There's a lot happening in parallel, but every problem can be traced down to a specific exchange between two nodes, consisting of a set of events that happen in strict sequence. We know how to make very sophisticated logging, but as usual it's wiser to make just what we need and no more. We have to capture:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Time and date for each event.</p></li><li class="listitem"><p>In which node the event occurred.</p></li><li class="listitem"><p>The peer node, if any.</p></li><li class="listitem"><p>What the event was (e.g., which command arrived).</p></li><li class="listitem"><p>Event data, if any.</p></li></ul></div><p>The very simplest technique is to print the necessary information to the console, with a timestamp. That's the approach I used. Then it's simple to find the nodes affected by a failure, filter the log file for only messages referring to them, and see exactly what happened.</p></div><div class="sect2" title="Dealing with Blocked Peers"><div class="titlepage"><div><div><h3 class="title"><a id="idp21416280"></a>Dealing with Blocked Peers</h3></div></div></div><p>In any performance-sensitive ØMQ architecture, you need to solve the problem of flow control. You cannot simply send unlimited messages to a socket and hope for the best. At the one extreme, you can exhaust memory. This is a classic failure pattern for a message broker: one slow client stops receiving messages; the broker starts to queue them, and eventually exhausts memory and the whole process dies. At the other extreme, the socket drops messages, or blocks, as you hit the high-water mark.</p><p>With Zyre we want to distribute messages to a set of peers, and we want to do this fairly. Using a single ROUTER socket for output would be problematic because any one blocked peer would block outgoing traffic to all peers. TCP does have good algorithms for spreading the network capacity across a set of connections. And we're using a separate DEALER socket to talk to each peer, so in theory each DEALER socket will send its queued messages in the background reasonably fairly.</p><p>The normal behavior of a DEALER socket that hits its high-water mark is to block. This is usually ideal, but it's a problem for us here. Our current interface design uses one thread that distributes messages to all peers. If one of those send calls were to block, all output would block.</p><p>There are a few options to avoid blocking. One is to use <code class="literal">zmq_poll()</code> on the whole set of DEALER sockets, and only write to sockets that are ready. I don't like this for a couple of reasons. First, the DEALER socket is hidden inside the peer class, and it is cleaner to allow each class to handle this opaquely. Second, what do we do with messages we can't yet deliver to a DEALER socket? Where do we queue them? Third, it seems to be side-stepping the issue. If a peer is really so busy it can't read its messages, something is wrong. Most likely, it's dead.</p><p>So no polling for output. The second option is to use one thread per peer. I quite like the idea of this because it fits into the ØMQ design pattern of "do one thing in one thread". But this is going to create <span class="emphasis"><em>a lot</em></span> of threads (square of the number of nodes we start) in the simulation, and we're already running out of file handles.</p><p>A third option is to use a nonblocking send. This is nicer and it's the solution I choose. We can then provide each peer with a reasonable outgoing queue (the HWM) and if that gets full, treat it as a fatal error on that peer. This will work for smaller messages. If we're sending large chunks--e.g., for content distribution--we'll need a credit-based flow control on top.</p><p>Therefore the first step is to prove to ourselves that we can turn the normal blocking DEALER socket into a nonblocking socket. This example creates a normal DEALER socket, connects it to some endpoint (so that there's an outgoing pipe and the socket will accept messages), sets the high-water mark to four, and then sets the send timeout to zero:</p><div class="example"><a id="eagain-c"></a><p class="title"><strong>Example 8.17. Checking EAGAIN on DEALER socket (eagain.c)</strong></p><div class="example-contents"><pre class="programlisting">
//  Shows how to provoke EAGAIN when reaching HWM

#include &lt;czmq.h&gt;

int main (void) {
    zctx_t *ctx = zctx_new ();
    
    void *mailbox = zsocket_new (ctx, ZMQ_DEALER);
    zsocket_set_sndhwm (mailbox, 4);
    zsocket_set_sndtimeo (mailbox, 0);
    zsocket_connect (mailbox, "tcp://localhost:9876");

    int count;
    for (count = 0; count &lt; 10; count++) {
        printf ("Sending message %d\n", count);
        int rc = zstr_send (mailbox, "message %d", count);
        if (rc == -1) {
            printf ("%s\n", strerror (errno));
            break;
        }
    }
    zctx_destroy (&amp;ctx);
    return 0;
}
</pre></div></div><br class="example-break" /><p>When we run this, we send four messages successfully (they go nowhere, the socket just queues them), and then we get a nice <code class="literal">EAGAIN</code> error:</p><pre class="screen">Sending message 0
Sending message 1
Sending message 2
Sending message 3
Sending message 4
Resource temporarily unavailable
</pre><p>The next step is to decide what a reasonable high-water mark would be for a peer. Zyre is meant for human interactions; that is, applications that chat at a low frequency, such as two games or a shared drawing program. I'd expect a hundred messages per second to be quite a lot. Our "peer is really dead" timeout is 10 seconds. So a high-water mark of 1,000 seems fair.</p><p>Rather than set a fixed HWM or use the default (which randomly also happens to be 1,000), we calculate it as 100 * the timeout. Here's how we configure a new DEALER socket for a peer:</p><pre class="programlisting">
//  Create new outgoing socket (drop any messages in transit)
self-&gt;mailbox = zsocket_new (self-&gt;ctx, ZMQ_DEALER);

//  Set our caller "From" identity so that receiving node knows
//  who each message came from.
zsocket_set_identity (self-&gt;mailbox, reply_to);

//  Set a high-water mark that allows for reasonable activity
zsocket_set_sndhwm (self-&gt;mailbox, PEER_EXPIRED * 100);

//  Send messages immediately or return EAGAIN
zsocket_set_sndtimeo (self-&gt;mailbox, 0);

//  Connect through to peer node
zsocket_connect (self-&gt;mailbox, "tcp://%s", endpoint);
</pre><p>And finally, what do we do when we get an <code class="literal">EAGAIN</code> on a peer? We don't need to go through all the work of destroying the peer because the interface will do this automatically if it doesn't get any message from the peer within the expiration timeout. Just dropping the last message seems very weak; it will give the receiving peer gaps.</p><p>I'd prefer a more brutal response. Brutal is good because it forces the design to a "good" or "bad" decision rather than a fuzzy "should work but to be honest there are a lot of edge cases so let's worry about it later". Destroy the socket, disconnect the peer, and stop sending anything to it. The peer will eventually have to reconnect and re-initialize any state. It's kind of an assertion that 100 messages a second is enough for anyone. So, in the <code class="literal">zre_peer_send</code> method:</p><pre class="programlisting">
int
zre_peer_send (zre_peer_t *self, zre_msg_t **msg_p)
{
    assert (self);
    if (self-&gt;connected) {
        if (zre_msg_send (msg_p, self-&gt;mailbox) &amp;&amp; errno == EAGAIN) {
            zre_peer_disconnect (self);
            return -1;
        }
    }
    return 0;
}
</pre><p>Where the disconnect method looks like this:</p><pre class="programlisting">
void
zre_peer_disconnect (zre_peer_t *self)
{
    //  If connected, destroy socket and drop all pending messages
    assert (self);
    if (self-&gt;connected) {
        zsocket_destroy (self-&gt;ctx, self-&gt;mailbox);
        free (self-&gt;endpoint);
        self-&gt;endpoint = NULL;
        self-&gt;connected = false;
    }
}
</pre></div></div><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="ch08s06.html">Prev</a> </td><td width="20%" align="center"><a accesskey="u" href="ch08.html">Up</a></td><td width="40%" align="right"> <a accesskey="n" href="ch08s08.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">Group Messaging </td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top"> Distributed Logging and Monitoring</td></tr></table></div></body></html>
