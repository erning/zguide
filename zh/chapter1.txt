.output chapter1.wd
.bookmark basics
+ 0MQ基础

++ 拯救世界

如何解释0MQ？有些人会先说一堆0MQ的好：//它是一套用于快速构建的套接字组件；它的信箱系统有超强的路由能力；它太快了！// 而有些人则喜欢分享他们被0MQ点悟的时刻，那些被灵感击中的瞬间：//所有的事情突然变得简单明了，让人大开眼界。// 另一些人则会拿0MQ同其他产品做个比较：//它更小，更简单，但却让人觉得如此熟悉。// 对于我个人而言，我则更倾向于和别人分享0MQ的诞生史，相信会和各位读者有所共鸣。

编程是一门科学，但往往会乔装成一门艺术。我们从不去了解软件最底层的机理，或者说根本没有人在乎这些。软件并不只是算法、数据结构、编程语言、或者抽象云云，这些不过是一些工具而已，被我们创造、使用、最后抛弃。软件真正的本质，其实是人的本质。

举例来说，当我们遇到一个高度复杂的问题时，我们会群策群力，分工合作，将问题拆分为若干个部分，一起解决。这里就体现了编程的科学：创建一组小型的构建模块，让人们易于理解和使用，那么大家就会一起用它来解决问题。

我们生活在一个普遍联系的世界里，需要现代的编程软件为我们做指引。所以，未来我们所需要的用于处理大规模计算的构建模块，必须是普遍联系的，而且能够并行运作。那时，程序代码不能再只关注自己，它们需要互相交流，变得足够健谈。程序代码需要像人脑一样，数以兆计的神经元高速地传输信号，在一个没有中央控制的环境下，没有单点故障的环境下，解决问题。这一点其实并不意外，因为就当今的网络来讲，每个节点其实就像是连接了一个人脑一样。

如果你曾和线程、协议、或网络打过交道，你会觉得我上面的话像是天方夜谭。因为在实际应用过程中，只是连接几个程序或网络就已经非常困难和麻烦了。数以兆计的节点？那真是无法想象的。现今只有资金雄厚的企业才能负担得起这种软件和服务。

当今世界的网络结构已经远远超越了我们自身的驾驭能力。十九世纪八十年代的软件危机，弗莱德•布鲁克斯曾说过，这个世上[http://en.wikipedia.org/wiki/No_Silver_Bullet 没有银弹]。

后来，免费和开源解决了这次软件危机，让我们能够高效地分享知识。如今，我们又面临一次新的软件危机，只不过我们谈论得不多。只有那些大型的、富足的企业才有财力建立高度联系的应用程序。那里有云的存在，但它是私有的。我们的数据和知识正在从我们的个人电脑中消失，流入云端，无法获得或与其竞争。是谁坐拥我们的社交网络？这真像一次巨型主机的革命。

我们暂且不谈其中的政治因素，光那些就可以另外出本书了。目前的现状是，虽然互联网能够让千万个程序相连，但我们之中的大多数却无法做到这些。这样一来，那些真正有趣的大型问题（如健康、教育、经济、交通等领域），仍然无法解决。我们没有能力将代码连接起来，也就不能像大脑中的神经元一样处理那些大规模的问题。

已经有人尝试用各种方法来连接应用程序，如数以千计的IETF规范，每种规范解决一个特定问题。对于开发人员来说，HTTP协议是比较简单和易用的，但这也往往让问题变得更糟，因为它鼓励人们形成一种重服务端、轻客户端的思想。

所以迄今为止人们还在使用原始的TCP/UDP协议、私有协议、HTTP协议、网络套接字等形式连接应用程序。这种做法依旧让人痛苦，速度慢又不易扩展，需要集中化管理。而分布式的P2P协议又仅仅适用于娱乐，而非真正的应用。有谁会使用Skype或者Bittorrent来交换数据呢？

这就让我们回归到编程科学的问题上来。想要拯救这个世界，我们需要做两件事情：一，如何在任何地点连接任何两个应用程序；二、将这个解决方案用最为简单的方式包装起来，供程序员使用。

也许这听起来太简单了，但事实确实如此。

++ 需要具备的知识

* 使用0MQ3.2或以上的版本；
* 使用Linux系统或其他相似的操作系统；
* 能够阅读C语言代码，这是本指南示例程序的默认语言；
* 当我们书写诸如PUSH或SUBSCRIBE等常量时，你能够找到相应语言的实现，如{{ZMQ_PUSH}}、{{ZMQ_SUBSCRIBE}}。

++ 获取示例

本指南的所有示例都存放于[https://github.com/imatix/zguide GitHub仓库]中，最简单的获取方式是运行以下代码：

[[code]]
git clone --depth=1 git://github.com/imatix/zguide.git
[[/code]]

浏览examples目录，你可以看到多种语言的实现。如果其中缺少了某种你正在使用的编程语言，我们很希望你可以[http://zguide.zeromq.org/main:translate 提交一份补充]。这也是本指南实用的原因，要感谢所有做出过贡献的人。

所有的示例代码都以MIT/X11协议发布，若在源代码中有其他限定的除外。

++ 提问-回答

让我们从简单的代码开始，一段传统的Hello World程序。我们会创建一个客户端和一个服务端，客户端发送Hello给服务端，服务端返回World。下文是C语言编写的服务端，它在5555端口打开一个0MQ套接字，等待请求，收到后应答World。

[[code type="example" title="Hello World server" name="hwserver" language="C"]]
[[/code]]

[[code type="textdiagram" title="Request-Reply"]]
  #------------#
  |   Client   |
  +------------+
  |    REQ     |
  '---+--------'
      |    ^
      |    |
Hello |    | World
      |    |
      v    |
  .--------+---.
  |    REP     |
  +------------+
  |   Server   |
  #------------#
[[/code]]

使用REQ-REP套接字发送和接受消息是需要遵循一定规律的。客户端首先使用{{zmq_send()}}发送消息，再用{{zmq_recv()}}接收，如此循环。如果打乱了这个顺序（如连续发送两次）则会报错。类似地，服务端必须先进行接收，后进行发送。

0MQ使用C语言作为它参考手册的语言，本指南也以它作为示例程序的语言。如果你正在阅读本指南的在线版本，你可以看到示例代码的下方有其他语言的实现。如以下是C++语言：

[[code type="example" title="Hello World server" name="hwserver" language="C++"]]
[[/code]]

可以看到C语言和C++语言的API代码差不多，而在PHP或Java这样的语言中，代码就会更为简洁：

[[code type="example" title="Hello World server" name="hwserver" language="PHP"]]
[[/code]]

[[code type="example" title="Hello World server" name="hwserver" language="Java"]]
[[/code]]

下面是客户端的代码：

[[code type="example" title="Hello World client" name="hwclient"]]
[[/code]]

这看起来是否太简单了？0MQ就是这样一个东西，你往里加点儿料就能制作出一枚无穷能量的原子弹，用它来拯救世界吧！理论上你可以连接千万个客户端到这个服务端上，同时连接都没问题，程序仍会运作得很好。你可以尝试一下先打开客户端，再打开服务端，可以看到程序仍然会正常工作，想想这意味着什么。

让我简单介绍一下这两段程序到底做了什么。首先，他们创建了一个0MQ上下文，然后是一个套接字。不要被这些陌生的名词吓到，后面我们都会讲到。服务端将REP套接字绑定到5555端口上，并开始等待请求，发出应答，如此循环。客户端则是发送请求并等待服务端的应答。

If you kill the server (Ctrl-C) and restart it, the client won't recover properly. Recovering from crashing processes isn't quite that easy. Making a reliable request-reply flow is complex enough that we won't cover it until [#reliable-request-reply].

这些代码背后其实发生了很多很多事情，但是程序员完全不必理会这些，只要知道这些代码短小精悍，极少出错，耐高压。这种通信模式我们称之为请求-应答模式，是ZMQ最直接的一种应用。你可以拿它和RPC及经典的C/S模型做类比。

++ 关于字符串

0MQ不会关心发送消息的内容，只要知道它所包含的字节数。所以，程序员需要做一些工作，保证对方节点能够正确读取这些消息。如何将一个对象或复杂数据类型转换成ZMQ可以发送的消息，这有类似Protocol Buffers的序列化软件可以做到。但对于字符串，你也是需要有所注意的。

在C语言中，字符串都以一个空字符结尾，你可以像这样发送一个完整的字符串：

[[code language="C"]]
zmq_send (requester, "Hello", 6, 0);
[[/code]]

但是，如果你用其他语言发送这个字符串，很可能不会包含这个空字节，如你使用Python发送：

[[code language="Python"]]
socket.send ("Hello")
[[/code]]

实际发送的消息是[figure]：

[[code type="textdiagram" title="A 0MQ string"]]
#-----#  #-----+-----+-----+-----+-----#
|  5  |  |  H  |  e  |  l  |  l  |  o  |
#-----#  #-----+-----+-----+-----+-----#
[[/code]]

如果你从C语言中读取该消息，你会读到一个类似于字符串的内容，甚至它可能就是一个字符串（第六位在内存中正好是一个空字符），但是这并不合适。这样一来，客户端和服务端对字符串的定义就不统一了，你会得到一些奇怪的结果。

当你用C语言从0MQ中获取字符串，你不能够相信该字符串有一个正确的结尾。因此，当你在接受字符串时，应该建立多一个字节的缓冲区，将字符串放进去，并添加结尾。

所以，让我们做如下假设：**0MQ的字符串是有长度的，且传送时//不加//结束符**。在最简单的情况下，0MQ字符串和0MQ消息中的一帧是等价的，就如上图所展现的，由一个长度属性和一串字节表示。

下面这个功能函数会帮助我们在C语言中正确的接受字符串消息：

[[code language="C"]]
//  Receive 0MQ string from socket and convert into C string
//  Chops string at 255 chars, if it's longer
static char *
s_recv (void *socket) {
    char buffer [256];
    int size = zmq_recv (socket, buffer, 255, 0);
    if (size == -1)
        return NULL;
    if (size > 255)
        size = 255;
    buffer [size] = 0;
    return strdup (buffer);
}
[[/code]]

这段代码我们会在日后的示例中使用，我们可以顺手写一个{{s_send()}}方法，并打包成一个.h文件供我们使用。

这就诞生了zhelpers.h，一个供C语言使用的0MQ功能函数库。它的源代码比较长，而且只对C语言程序员有用，你可以在闲暇时[https://github.com/imatix/zguide/blob/master/examples/C/zhelpers.h 看一看]。

++ 获取版本号

0MQ目前有多个版本，而且仍在持续更新。如果你遇到了问题，也许这在下一个版本中已经解决了。想知道目前的0MQ版本，你可以在程序中运行如下：

[[code type="example" title="0MQ version reporting" name="version"]]
[[/code]]

++ 让消息流动起来

第二种经典的消息模式是单向数据分发：服务端将更新事件发送给一组客户端。让我们看一个天气信息发布的例子，包括邮编、温度、相对湿度。我们随机生成这些信息，气象站好像也是这么干的。

下面是服务端的代码，使用5556端口：

[[code type="example" title="Weather update server" name="wuserver"]]
[[/code]]

这项更新服务没有开始、没有结束，就像永不消失的电波一样[figure]。

下面是客户端程序，它会接受发布者的消息，只处理特定邮编标注的信息，如纽约的邮编是10001:

[[code type="example" title="Weather update client" name="wuclient"]]
[[/code]]

[[code type="textdiagram" title="Publish-Subscribe"]]
               #-------------#
               |  Publisher  |
               +-------------+
               |     PUB     |
               '-------------'
                    bind
                      |
                      |
                   updates
                      |
      .---------------+---------------.
      |               |               |
   updates         updates         updates
      |               |               |
      |               |               |
      v               v               v
   connect         connect         connect
.------------.  .------------.  .------------.
|    SUB     |  |    SUB     |  |    SUB     |
+------------+  +------------+  +------------+
| Subscriber |  | Subscriber |  | Subscriber |
#------------#  #------------#  #------------#
[[/code]]

需要注意的是，在使用SUB套接字时，必须使用{{zmq_setsockopt()}}方法来设置订阅的内容。如果你不设置订阅内容，那将什么消息都收不到，新手很容易犯这个错误。订阅信息可以是任何字符串，可以设置多次。只要消息满足其中一条订阅信息，SUB套接字就会收到。订阅者可以选择不接收某类消息，也是通过{{zmq_setsockopt()}}方法实现的。

PUB-SUB套接字组合是异步的。客户端在一个循环体中使用{{zmq_recv()}}接收消息，如果向SUB套接字发送消息则会报错；类似地，服务端可以不断地使用{{zmq_send()}}发送消息，但不能再PUB套接字上使用{{zme_recv()}}。

In theory with 0MQ sockets, it does not matter which end connects and which end binds. However, in practice there are undocumented differences that I'll come to later. For now, bind the PUB and connect the SUB, unless your network design makes that impossible.

关于PUB-SUB套接字，还有一点需要注意：你无法得知SUB是何时开始接收消息的。就算你先打开了SUB套接字，后打开PUB发送消息，这时**SUB还是会丢失一些消息**的，因为建立连接是需要一些时间的。很少，但并不是零。

这种“慢连接”的症状一开始会让很多人困惑，所以这里我要详细解释一下。还记得ZMQ是在后台进行异步的I/O传输的，如果你有两个节点用以下顺序相连：

* 订阅者连接至端点接收消息并计数；
* 发布者绑定至端点并立刻发送1000条消息。

运行的结果很可能是订阅者一条消息都收不到。这时你可能会傻眼，忙于检查有没有设置订阅信息，并重新尝试，但结果还是一样。

我们知道在建立TCP连接时需要进行三次握手，会耗费几毫秒的时间，而当节点数增加时这个数字也会上升。在这么短的时间里，0MQ就可以发送很多很多消息了。举例来说，如果建立连接需要耗时5毫秒，而0MQ只需要1毫秒就可以发送完这1000条消息。

[#sockets-and-patterns] 中我会解释如何使发布者和订阅者同步，只有当订阅者准备好时发布者才会开始发送消息。有一种简单的方法来同步PUB和SUB，就是让PUB延迟一段时间再发送消息。现实编程中我不建议使用这种方式，因为它太脆弱了，而且不好控制。不过这里我们先暂且使用sleep的方式来解决，等到 [#sockets-and-patterns] 的时候再讲述正确的处理方式。

另一种同步的方式则是认为发布者的消息流是无穷无尽的，因此丢失了前面一部分信息也没有关系。我们的气象信息客户端就是这么做的。

示例中的气象信息客户端会收集指定邮编的一百条信息，其间大约有1000万条信息被发布。你可以先打开客户端，再打开服务端，工作一段时间后重启服务端，这时客户端仍会正常工作。当客户端收集完所需信息后，会计算并输出平均温度。

关于发布-订阅模式的几点说明：

* 订阅者可以连接多个发布者，轮流接收消息；

* 如果发布者没有订阅者与之相连，那它发送的消息将直接被丢弃；

* 如果你使用TCP协议，那当订阅者处理速度过慢时，消息会在发布者处堆积。以后我们会讨论如何使用阈值（HWM）来保护发布者。

* 从0MQ v3.x开始，当使用{{tcp://}}或{{ipc://}}协议时，消息的过滤是在发布者处进行的当使用{{epgm://}}}协议时，消息的过滤是在订阅者处进行的；在0MQ v2.x中，消息的过滤总是在订阅者处进行的。

我在自己的计算机（2011-era Intel i5）上尝试发布1000万条消息，速度很快，但没什么特别的：

[[code]]
$ time wuclient
Collecting updates from weather server...
Average temperature for zipcode '10001 ' was 28F

real    0m4.470s
user    0m0.000s
sys     0m0.008s
[[/code]]

++ 分布式处理

[[code type="textdiagram" title="Parallel Pipeline"]]
            #-------------#
            |  Ventilator |
            +-------------+
            |    PUSH     |
            '------+------'
                   |
                 tasks
                   |
      .------------+-------------.
      |            |             |
      v            v             v
.----------.  .----------.  .----------.
|   PULL   |  |   PULL   |  |   PULL   |
+----------+  +----------+  +----------+
|  Worker  |  |  Worker  |  |  Worker  |
+----------+  +----------+  +----------+
|   PUSH   |  |   PUSH   |  |   PUSH   |
'----+-----'  '----+-----'  '----+-----'
      |            |             |
      '------------+-------------'
                   |
                results
                   |
                   v
            .-------------.
            |    PULL     |
            +-------------+
            |    Sink     |
            #-------------#
[[/code]]

As a final example (you are surely getting tired of juicy code and want to delve back into philological discussions about comparative abstractive norms), let's do a little supercomputing. Then coffee. Our supercomputing application is a fairly typical parallel processing model[figure]. We have:

* 任务分发器会生成大量可以并行计算的任务；
* 有一组worker会处理这些任务；
* 结果收集器会在末端接收所有worker的处理结果，进行汇总。

现实中，worker可能散落在不同的计算机中，利用GPU（图像处理单元）进行复杂计算。下面是任务分发器的代码，它会生成100个任务，任务内容是让收到的worker延迟若干毫秒。

[[code type="example" title="Parallel task ventilator" name="taskvent"]]
[[/code]]

下面是worker的代码，它接受信息并延迟指定的毫秒数，并发送执行完毕的信号：

[[code type="example" title="Parallel task worker" name="taskwork"]]
[[/code]]

下面是结果收集器的代码。它会收集100个处理结果，并计算总的执行时间，让我们由此判别任务是否是并行计算的。

[[code type="example" title="Parallel task sink" name="tasksink"]]
[[/code]]

一组任务的平均执行时间在5秒左右，以下是分别开始1个、2个、4个worker时的执行结果：

* 1 worker: total elapsed time: 5034 msecs.
* 2 workers: total elapsed time: 2421 msecs.
* 4 workers: total elapsed time: 1018 msecs.

关于这段代码的几个细节：

* worker上游和任务分发器相连，下游和结果收集器相连，这就意味着你可以开启任意多个worker。但若worker是绑定至端点的，而非连接至端点，那我们就需要准备更多的端点，并配置任务分发器和结果收集器。所以说，任务分发器和结果收集器是这个网络结构中较为//稳定//的部分，因此应该由它们绑定至端点，而非worker，因为它们较为//动态//。

* 我们需要做一些同步的工作，等待worker全部启动之后再分发任务。这点在0MQ中很重要，且不易解决。连接套接字的动作会耗费一定的时间，因此当第一个worker连接成功时，它会一下收到很多任务。所以说，如果我们不进行同步，那这些任务根本就不会被并行地执行。你可以自己试验一下。

* 任务分发器使用PUSH套接字向worker均匀地分发任务（假设所有的worker都已经连接上了），这种机制称为//负载均衡//，以后我们会见得更多。

* 结果收集器的PULL套接字会均匀地从worker处收集消息，这种机制称为//公平队列//[figure]：

[[code type="textdiagram" title="Fair Queuing"]]
#---------#   #---------#   #---------#
|  PUSH   |   |  PUSH   |   |  PUSH   |
'----+----'   '----+----'   '----+----'
     |             |             |
 R1, R2, R3       R4           R5, R6
     |             |             |
     '-------------+-------------'
                   |
             fair-queuing
         R1, R4, R5, R2, R6, R3
                   |
                   v
            .-------------.
            |     PULL    |
            #-------------#
[[/code]]

管道模式也会出现慢连接的情况，让人误以为PUSH套接字没有进行负载均衡。如果你的程序中某个worker接收到了更多的请求，那是因为它的PULL套接字连接得比较快，从而在别的worker连接之前获取了额外的消息。

If you want proper load balancing, you probably want to look at the The load balancing pattern in [#advanced-request-reply].

++ 使用0MQ编程

看着这些示例程序后，你一定迫不及待想要用0MQ进行编程了。不过在开始之前，我还有几条建议想给到你，这样可以省去未来的一些麻烦：

* 学习0MQ要循序渐进，虽然它只是一套API，但却提供了无尽的可能。一步一步学习它提供的功能，并完全掌握。

* 编写漂亮的代码。丑陋的代码会隐藏问题，让想要帮助你的人无从下手。比如，你会习惯于使用无意义的变量名，但读你代码的人并不知道。应使用有意义的变量名称，而不是随意起一个。代码的缩进要统一，布局清晰。漂亮的代码可以让你的世界变得更美好。

* 边写边测试，当代码出现问题，你就可以快速定位到某些行。这一点在编写0MQ应用程序时尤为重要，因为很多时候你//无法//第一次就编写出正确的代码。

* 当你发现自己编写的代码无法正常工作时，你可以将其拆分成一些代码片段，看看哪段没有正确地执行。0MQ可以让你构建非常模块化的代码，所以应该好好利用这一点。

* 需要时应使用抽象的方法来编写程序（类、成员函数等等），不要随意拷贝代码，因为拷贝代码的同时也是在拷贝错误。

+++ 正确地使用上下文

0MQ applications always start by creating a //context//, and then using that for creating sockets. In C, it's the {{zmq_ctx_new[3]}} call. You should create and use exactly one context in your process. Technically, the context is the container for all sockets in a single process, and acts as the transport for {{inproc}} sockets, which are the fastest way to connect threads in one process. If at runtime a process has two contexts, these are like separate 0MQ instances. If that's explicitly what you want, OK, but otherwise remember:

**Do one {{zmq_ctx_new[3]}} at the start of your main line code, and one {{zmq_ctx_destroy[3]}} at the end.**

If you're using the {{fork()}} system call, each process needs its own context. If you do {{zmq_ctx_new[3]}} in the main process before calling {{fork()}}, the child processes get their own contexts. In general, you want to do the interesting stuff in the child processes and just manage these from the parent process.

+++ 正确地退出和清理

Classy programmers share the same motto as classy hit men: always clean-up when you finish the job. When you use 0MQ in a language like Python, stuff gets automatically freed for you. But when using C, you have to carefully free objects when you're finished with them or else you get memory leaks, unstable applications, and generally bad karma.

Memory leaks are one thing, but 0MQ is quite finicky about how you exit an application. The reasons are technical and painful, but the upshot is that if you leave any sockets open, the {{zmq_ctx_destroy[3]}} function will hang forever. And even if you close all sockets, {{zmq_ctx_destroy[3]}} will by default wait forever if there are pending connects or sends unless you set the LINGER to zero on those sockets before closing them.

The 0MQ objects we need to worry about are messages, sockets, and contexts. Luckily it's quite simple, at least in simple programs:

* Use {{zmq_send[3]}} and {{zmq_recv[3]}} when you can, as it avoids the need to work with zmq_msg_t objects.

* If you do use {{zmq_msg_recv[3]}}, always release the received message as soon as you're done with it, by calling {{zmq_msg_close[3]}}.

* If you are opening and closing a lot of sockets, that's probably a sign that you need to redesign your application. In some cases socket handles won't be freed until you destroy the context.

* When you exit the program, close your sockets and then call {{zmq_ctx_destroy[3]}}. This destroys the context.

This is at least the case for C development. In a language with automatic object destruction, sockets and contexts will be destroyed as you leave the scope. If you use exceptions you'll have to do the clean-up in something like a "final" block, the same as for any resource.

If you're doing multithreaded work, it gets rather more complex than this. We'll get to multithreading in the next chapter, but because some of you will, despite warnings, try to run before you can safely walk, below is the quick and dirty guide to making a clean exit in a //multithreaded// 0MQ application.

First, do not try to use the same socket from multiple threads. Please don't explain why you think this would be excellent fun, just please don't do it. Next, you need to shut down each socket that has ongoing requests. The proper way is to set a low LINGER value (1 second), and then close the socket. If your language binding doesn't do this for you automatically when you destroy a context, I'd suggest sending a patch.

Finally, destroy the context. This will cause any blocking receives or polls or sends in attached threads (i.e., which share the same context) to return with an error. Catch that error, and then set linger on, and close sockets in //that// thread, and exit. Do not destroy the same context twice. The {{zmq_ctx_destroy}} in the main thread will block until all sockets it knows about are safely closed.

Voila! It's complex and painful enough that any language binding author worth his or her salt will do this automatically and make the socket closing dance unnecessary.

++ 我们为什么需要0MQ

现在我们已经将0MQ运行起来了，让我们回顾一下为什么我们需要0MQ：

目前的应用程序很多都会包含跨网络的组件，无论是局域网还是因特网。这些程序的开发者都会用到某种消息通信机制。有些人会使用某种消息队列产品，而大多数人则会自己手工来做这些事，使用TCP或UDP协议。这些协议使用起来并不困难，但是，简单地将消息从A发给B，和在任何情况下都能进行可靠的消息传输，这两种情况显然是不同的。

让我们看看在使用纯TCP协议进行消息传输时会遇到的一些典型问题。任何可复用的消息传输层肯定或多或少地会要解决以下问题：

* 如何处理I/O？是让程序阻塞等待响应，还是在后台处理这些事？这是软件设计的关键因素。阻塞式的I/O操作会让程序架构难以扩展，而后台处理I/O也是比较困难的。

* 如何处理那些临时的、来去自由的组件？我们是否要将组件分为客户端和服务端两种，并要求服务端永不消失？那如果我们想要将服务端相连怎么办？我们要每隔几秒就进行重连吗？

* 我们如何表示一条消息？我们怎样通过拆分消息，让其变得易读易写，不用担心缓存溢出，既能高效地传输小消息，又能胜任视频等大型文件的传输？

* 如何处理那些不能立刻发送出去的消息？比如我们需要等待一个网络组件重新连接的时候？我们是直接丢弃该条消息，还是将它存入数据库，或是内存中的一个队列？

* 要在哪里保存消息队列？如果某个组件读取消息队列的速度很慢，造成消息的堆积怎么办？我们要采取什么样的策略？

* 如何处理丢失的消息？我们是等待新的数据，请求重发，还是需要建立一套新的可靠性机制以保证消息不会丢失？如果这个机制自身崩溃了呢？

* 如果我们想换一种网络连接协议，如用广播代替TCP单播？或者改用IPv6？我们是否需要重写所有的应用程序，或者将这种协议抽象到一个单独的层中？

* 我们如何对消息进行路由？我们可以将消息同时发送给多个节点吗？是否能将应答消息返回给请求的发送方？

* 我们如何为另一种语言写一个API？我们是否需要完全重写某项协议，还是重新打包一个类库？

* 怎样才能做到在不同的架构之间传送消息？是否需要为消息规定一种编码？

* 我们如何处理网络通信错误？等待并重试，还是直接忽略或取消？

我们可以找一个开源软件来做例子，如[http://hadoop.apache.org/zookeeper/ Hadoop Zookeeper]，看一下它的C语言API源码，{{[http://github.com/apache/zookeeper/blob/trunk/src/c/src/zookeeper.c src/c/src/zookeeper.c]}}。2013年1月时，这段代码大约有4,200行，没有注释，实现了一个C/S网络通信协议。它工作起来很高效，因为使用了{{poll()}}来代替{{select()}}。但是，Zookeeper应该被抽象出来，作为一种通用的消息通信层，并加以详细的注释。像这样的模块应该得到最大程度上的复用，而不是重复地制造轮子。

但是，如何编写这样一个可复用的消息层呢？为什么长久以来人们宁愿在自己的代码中重复书写控制原始TCP套接字的代码，而不愿编写这样一个公共库呢？
[figure]?

其实，要编写一个通用的消息层是件非常困难的事，这也是为什么FOSS项目不断在尝试，一些商业化的消息产品如此之复杂、昂贵、僵硬、脆弱。2006年，iMatix设计了[http://www.amqp.org AMQP]协议，为FOSS项目的开发者提供了可能是当时第一个可复用的消息系统。AMQP比其他同类产品要来得好，但(http://www.imatix.com/articles:whats-wrong-with-amqp 仍然是复杂、昂贵和脆弱的)。它需要花费几周的时间去学习，花费数月的时间去创建一个真正能用的架构，到那时可能为时已晚了。

[[code type="textdiagram" title="Messaging as it Starts"]]
.------------.
|            |
|  Piece A   |
|            |
'------------'
      ^
      |
     TCP
      |
      v
.------------.
|            |
|  Piece B   |
|            |
'------------'
[[/code]]

Most messaging projects, like AMQP, that try to solve this long list of problems in a reusable way do so by inventing a new concept, the "broker", that does addressing, routing, and queuing. This results in a client/server protocol or a set of APIs on top of some undocumented protocol that allows applications to speak to this broker. Brokers are an excellent thing in reducing the complexity of large networks. But adding broker-based messaging to a product like Zookeeper would make it worse, not better. It would mean adding an additional big box, and a new single point of failure. A broker rapidly becomes a bottleneck and a new risk to manage. If the software supports it, we can add a second, third, and fourth broker and make some failover scheme. People do this. It creates more moving pieces, more complexity, and more things to break.

And a broker-centric setup needs its own operations team. You literally need to watch the brokers day and night, and beat them with a stick when they start misbehaving. You need boxes, and you need backup boxes, and you need people to manage those boxes. It is only worth doing for large applications with many moving pieces, built by several teams of people over several years.

[[code type="textdiagram" title="Messaging as it Becomes"]]
        .---.             .---.
.---.   |   |   .---.  ^  |   |
|   +-->|   |<--|   |  |  |   |
|   |   '---'   |   |  |  '-+-'
'-+-'           '-+-'  |    |
  |               ^    |    |
  |       .-------+----+----'
  |       |       |    |
  '-------+-------+----+--.
          |       |    |  |
  .-------+-------+----+--+-----.
  |       v       |       v     |
.-+-.   .---.     |     .---.   |
|   |   |   |   .-+-.   |   |-->|
|   +-->|   +-->|   +-->|   |   |
'---'   '---'   |   |   '---'   |
          ^     '-+-'     ^     |
          |       |       |     |
  .-------+-------+-------'     |
  |       |       |             |
  v     .-+-.     v     .---.   |
.---.   |   |   .---.   |   |   |
|   |<--|   |<--|   |<--|   |<--'
|   |   '---'   |   |   '---'
'---'           '---'
[[/code]]

So small to medium application developers are trapped. Either they avoid network programming and make monolithic applications that do not scale. Or they jump into network programming and make brittle, complex applications that are hard to maintain. Or they bet on a messaging product, and end up with scalable applications that depend on expensive, easily broken technology. There has been no really good choice, which is maybe why messaging is largely stuck in the last century and stirs strong emotions: negative ones for users, gleeful joy for those selling support and licenses[figure].

我们真正需要的是这样一种消息软件，它能够做大型消息软件所能做的一切，但使用起来又非常简单，成本很低，可以用到所有的应用程序中，没有任何依赖条件。因为没有了额外的模块，就降低了出错的概率。这种软件需要能够在所有的操作系统上运行，并能支持所有的编程语言。

0MQ就是这样一种软件：它高效，提供了嵌入式的类库，使应用程序能够很好地在网络中扩展，成本低廉。

0MQ主要特点有：

* 0MQ会在后台线程异步地处理I/O操作，它使用一种不会死锁的数据结构来存储消息。

* 网络组件可以来去自如，0MQ会负责自动重连，这就意味着你可以以任何顺序启动组件；用它创建的面向服务架构（SOAs）中，服务端可以随意地加入或退出网络。

* 0MQ会在有必要的情况下自动将消息放入队列中保存，一旦建立了连接就开始发送。

* 0MQ有阈值（HWM）的机制，可以避免消息溢出。当队列已满，0MQ会自动阻塞发送者，或丢弃部分消息，这些行为取决于你所使用的消息模式。

* 0MQ可以让你用不同的通信协议进行连接，如TCP、广播、进程内、进程间。改变通信协议时你不需要去修改代码。

* 0MQ会恰当地处理速度较慢的节点，会根据消息模式使用不同的策略。

* 0MQ提供了多种模式进行消息路由，如请求-应答模式、发布-订阅模式等。这些模式可以用来搭建网络拓扑结构。

* 0MQ中可以根据消息模式建立起一些中间装置（很小巧），可以用来降低网络的复杂程度。

* 0MQ会发送整个消息，使用消息帧的机制来传递。如果你发送了10KB大小的消息，你就会收到10KB大小的消息。

* 0MQ不强制使用某种消息格式，消息可以是0字节的，或是大到GB级的数据。当你表示这些消息时，可以选用诸如谷歌的protocol buffers，XDR等序列化产品。

* 0MQ能够智能地处理网络错误，有时它会进行重试，有时会告知你某项操作发生了错误。

* 0MQ甚至可以降低对环境的污染，因为节省了CPU时间意味着节省了电能。

其实0MQ可以做的还不止这些，它会颠覆人们编写网络应用程序的模式。虽然从表面上看，它不过是提供了一套处理套接字的API，能够用zmq_recv()和zmq_send()进行消息的收发，但是，消息处理将成为应用程序的核心部分，很快你的程序就会变成一个个消息处理模块，这既美观又自然。它的扩展性还很强，每项任务由一个节点（节点是一个线程）、同一台机器上的两个节点（节点是一个进程）、同一网络上的两台机器（节点是一台机器）来处理，而不需要改动应用程序。


++ 套接字的扩展性

我们来用实例看看0MQ套接字的扩展性。这个脚本会启动气象信息服务及多个客户端：

[[code]]
wuserver &
wuclient 12345 &
wuclient 23456 &
wuclient 34567 &
wuclient 45678 &
wuclient 56789 &
[[/code]]

执行过程中，我们可以通过{{top}}命令查看进程状态（以下是一台四核机器的情况）：

[[code]]
PID  USER  PR  NI  VIRT  RES  SHR S %CPU %MEM   TIME+  COMMAND
7136  ph   20   0 1040m 959m 1156 R  157 12.0 16:25.47 wuserver
7966  ph   20   0 98608 1804 1372 S   33  0.0  0:03.94 wuclient
7963  ph   20   0 33116 1748 1372 S   14  0.0  0:00.76 wuclient
7965  ph   20   0 33116 1784 1372 S    6  0.0  0:00.47 wuclient
7964  ph   20   0 33116 1788 1372 S    5  0.0  0:00.25 wuclient
7967  ph   20   0 33072 1740 1372 S    5  0.0  0:00.35 wuclient
[[/code]]

我们想想现在发生了什么：气象信息服务程序有一个单独的套接字，却能同时向五个客户端并行地发送消息。我们可以有成百上千个客户端并行地运作，服务端看不到这些客户端，不能操纵它们。So the 0MQ socket is acting like a little server, silently accepting client requests and shoving data out to them as fast as the network can handle it. And it's a multithreaded server, squeezing more juice out of your CPU.

++ Upgrading from 0MQ v2.2 to 0MQ v3.2

+++ Compatible Changes

These changes don't impact existing application code directly:

* Pub-sub filtering is now done at the publisher side instead of subscriber side. This improves performance significantly in many pub-sub use cases. You can mix v3.2 and v2.1/v2.2 publishers and subscribers safely.

* 0MQ v3.2 has many new API methods ({{zmq_disconnect[3]}}, {{zmq_unbind[3]}}, {{zmq_monitor[3]}}, {{zmq_ctx_set[3]}}, etc.)

+++ Incompatible Changes

These are the main areas of impact on applications and language bindings:

* Changed send/recv methods: {{zmq_send[3]}} and {{zmq_recv[3]}} have a different, simpler interface, and the old functionality is now provided by {{zmq_msg_send[3]}} and {{zmq_msg_recv[3]}}. Symptom: compile errors. Solution: fix up your code.

* These two methods return positive values on success, and -1 on error. In v2.x they always returned zero on success. Symptom: apparent errors when things actually work fine. Solution: test strictly for return code = -1, not non-zero.

* {{zmq_poll[3]}} now waits for milliseconds, not microseconds. Symptom: application stops responding (in fact responds 1000 times slower). Solution: use the {{ZMQ_POLL_MSEC}} macro defined below, in all {{zmq_poll}} calls.

* {{ZMQ_NOBLOCK}} is now called {{ZMQ_DONTWAIT}}. Symptom: compile failures on the {{ZMQ_NOBLOCK}} macro.

* The {{ZMQ_HWM}} socket option is now broken into {{ZMQ_SNDHWM}} and {{ZMQ_RCVHWM}}.  Symptom: compile failures on the {{ZMQ_HWM}} macro.

* Most but not all {{zmq_getsockopt[3]}} options are now integer values. Symptom: runtime error returns on {{zmq_setsockopt}} and {{zmq_getsockopt}}.

* The {{ZMQ_SWAP}} option has been removed. Symptom: compile failures on {{ZMQ_SWAP}}. Solution: redesign any code that uses this functionality.

+++ Suggested Shim Macros

For applications that want to run on both v2.x and v3.2, such as language bindings, our advice is to emulate c3.2 as far as possible. Here are C macro definitions that help your C/C++ code to work across both versions (taken from [http://czmq.zeromq.org CZMQ]):

[[code type="fragment" name="upgrade-shim"]]
#ifndef ZMQ_DONTWAIT
#   define ZMQ_DONTWAIT     ZMQ_NOBLOCK
#endif
#if ZMQ_VERSION_MAJOR == 2
#   define zmq_msg_send(msg,sock,opt) zmq_send (sock, msg, opt)
#   define zmq_msg_recv(msg,sock,opt) zmq_recv (sock, msg, opt)
#   define zmq_ctx_destroy(context) zmq_term(context)
#   define ZMQ_POLL_MSEC    1000        //  zmq_poll is usec
#   define ZMQ_SNDHWM ZMQ_HWM
#   define ZMQ_RCVHWM ZMQ_HWM
#elif ZMQ_VERSION_MAJOR == 3
#   define ZMQ_POLL_MSEC    1           //  zmq_poll is msec
#endif
[[/code]]

++ Warning: Unstable Paradigms!

Traditional network programming is built on the general assumption that one socket talks to one connection, one peer. There are multicast protocols, but these are exotic. When we assume "one socket = one connection", we scale our architectures in certain ways. We create threads of logic where each thread work with one socket, one peer. We place intelligence and state in these threads.

In the 0MQ universe, sockets are doorways to fast little background communications engines that manage a whole set of connections automagically for you. You can't see, work with, open, close, or attach state to these connections. Whether you use blocking send or receive, or poll, all you can talk to is the socket, not the connections it manages for you. The connections are private and invisible, and this is the key to 0MQ's scalability.

This is because your code, talking to a socket, can then handle any number of connections across whatever network protocols are around, without change. A messaging pattern sitting in 0MQ scales more cheaply than a messaging pattern sitting in your application code.

So the general assumption no longer applies. As you read the code examples, your brain will try to map them to what you know. You will read "socket" and think "ah, that represents a connection to another node". That is wrong. You will read "thread" and your brain will again think, "ah, a thread represents a connection to another node", and again your brain will be wrong.

If you're reading this Guide for the first time, realize that until you actually write 0MQ code for a day or two (and maybe three or four days), you may feel confused, especially by how simple 0MQ makes things for you, and you may try to impose that general assumption on 0MQ, and it won't work. And then you will experience your moment of enlightenment and trust, that //zap-pow-kaboom// satori paradigm-shift moment when it all becomes clear.
