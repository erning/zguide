<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>High-Speed Subscribers (Black Box Pattern)</title><meta name="generator" content="DocBook XSL Stylesheets V1.76.1" /><link rel="home" href="index.html" title="The ZeroMQ Guide - for C Developers" /><link rel="up" href="ch05.html" title="Chapter 5. Advanced Pub-Sub Patterns" /><link rel="prev" href="ch05s04.html" title="Slow Subscriber Detection (Suicidal Snail Pattern)" /><link rel="next" href="ch05s06.html" title="Reliable Pub-Sub (Clone Pattern)" /></head><body><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">High-Speed Subscribers (Black Box Pattern)</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="ch05s04.html">Prev</a> </td><th width="60%" align="center">Chapter 5. Advanced Pub-Sub Patterns</th><td width="20%" align="right"> <a accesskey="n" href="ch05s06.html">Next</a></td></tr></table><hr /></div><div class="sect1" title="High-Speed Subscribers (Black Box Pattern)"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="idp20226816"></a>High-Speed Subscribers (Black Box Pattern)</h2></div></div></div><p>Now lets look at one way to make our subscribers faster. A common use case for pub-sub is distributing large data streams like market data coming from stock exchanges. A typical setup would have a publisher connected to a stock exchange, taking price quotes, and sending them out to a number of subscribers. If there are a handful of subscribers, we could use TCP. If we have a larger number of subscribers, we'd probably use reliable multicast, i.e., PGM.</p><div class="figure"><a id="figure-56"></a><p class="title"><strong>Figure 5.1. The Simple Black Box Pattern</strong></p><div class="figure-contents"><div class="mediaobject"><img src="images/fig56.png" alt="The Simple Black Box Pattern" /></div></div></div><br class="figure-break" /><p>Let's imagine our feed has an average of 100,000 100-byte messages a second. That's a typical rate, after filtering market data we don't need to send on to subscribers. Now we decide to record a day's data (maybe 250 GB in 8 hours), and then replay it to a simulation network, i.e., a small group of subscribers. While 100K messages a second is easy for a ØMQ application, we want to replay it <span class="emphasis"><em>much faster</em></span>.</p><p>So we set up our architecture with a bunch of boxes--one for the publisher and one for each subscriber. These are well-specified boxes--eight cores, twelve for the publisher.</p><p>And as we pump data into our subscribers, we notice two things:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>When we do even the slightest amount of work with a message, it slows down our subscriber to the point where it can't catch up with the publisher again.</p></li><li class="listitem"><p>We're hitting a ceiling, at both publisher and subscriber, to around 6M messages a second, even after careful optimization and TCP tuning.</p></li></ol></div><p>The first thing we have to do is break our subscriber into a multithreaded design so that we can do work with messages in one set of threads, while reading messages in another. Typically, we don't want to process every message the same way. Rather, the subscriber will filter some messages, perhaps by prefix key. When a message matches some criteria, the subscriber will call a worker to deal with it. In ØMQ terms, this means sending the message to a worker thread.</p><p>So the subscriber looks something like a queue device. We could use various sockets to connect the subscriber and workers. If we assume one-way traffic and workers that are all identical, we can use PUSH and PULL and delegate all the routing work to ØMQ<a class="xref" href="ch05s05.html#figure-57" title="Figure 5.2. Mad Black Box Pattern">Figure 5.2, “Mad Black Box Pattern”</a>. This is the simplest and fastest approach.</p><p>The subscriber talks to the publisher over TCP or PGM. The subscriber talks to its workers, which are all in the same process, over <code class="literal">inproc://</code>.</p><div class="figure"><a id="figure-57"></a><p class="title"><strong>Figure 5.2. Mad Black Box Pattern</strong></p><div class="figure-contents"><div class="mediaobject"><img src="images/fig57.png" alt="Mad Black Box Pattern" /></div></div></div><br class="figure-break" /><p>Now to break that ceiling. The subscriber thread hits 100% of CPU and because it is one thread, it cannot use more than one core. A single thread will always hit a ceiling, be it at 2M, 6M, or more messages per second. We want to split the work across multiple threads that can run in parallel.</p><p>The approach used by many high-performance products, which works here, is <span class="emphasis"><em>sharding</em></span>. Using sharding, we split the work into parallel and independent streams, such as half of the topic keys in one stream, and half in another. We could use many streams, but performance won't scale unless we have free cores. So let's see how to shard into two streams<a class="xref" href="ch05s06.html#figure-58" title="Figure 5.3. Publishing State Updates">Figure 5.3, “Publishing State Updates”</a>.</p><p>With two streams, working at full speed, we would configure ØMQ as follows:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Two I/O threads, rather than one.</p></li><li class="listitem"><p>Two network interfaces (NIC), one per subscriber.</p></li><li class="listitem"><p>Each I/O thread bound to a specific NIC.</p></li><li class="listitem"><p>Two subscriber threads, bound to specific cores.</p></li><li class="listitem"><p>Two SUB sockets, one per subscriber thread.</p></li><li class="listitem"><p>The remaining cores assigned to worker threads.</p></li><li class="listitem"><p>Worker threads connected to both subscriber PUSH sockets.</p></li></ul></div><p>Ideally, we want to match the number of fully-loaded threads in our architecture with the number of cores. When threads start to fight for cores and CPU cycles, the cost of adding more threads outweighs the benefits. There would be no benefit, for example, in creating more I/O threads.</p></div><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="ch05s04.html">Prev</a> </td><td width="20%" align="center"><a accesskey="u" href="ch05.html">Up</a></td><td width="40%" align="right"> <a accesskey="n" href="ch05s06.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">Slow Subscriber Detection (Suicidal Snail Pattern) </td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top"> Reliable Pub-Sub (Clone Pattern)</td></tr></table></div></body></html>
