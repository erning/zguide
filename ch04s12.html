<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Disconnected Reliability (Titanic Pattern)</title><meta name="generator" content="DocBook XSL Stylesheets V1.76.1" /><link rel="home" href="index.html" title="The ZeroMQ Guide - for C Developers" /><link rel="up" href="ch04.html" title="Chapter 4. Reliable Request-Reply Patterns" /><link rel="prev" href="ch04s11.html" title="Idempotent Services" /><link rel="next" href="ch04s13.html" title="High-Availability Pair (Binary Star Pattern)" /></head><body><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">Disconnected Reliability (Titanic Pattern)</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="ch04s11.html">Prev</a> </td><th width="60%" align="center">Chapter 4. Reliable Request-Reply Patterns</th><td width="20%" align="right"> <a accesskey="n" href="ch04s13.html">Next</a></td></tr></table><hr /></div><div class="sect1" title="Disconnected Reliability (Titanic Pattern)"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="idp19926120"></a>Disconnected Reliability (Titanic Pattern)</h2></div></div></div><p>Once you realize that Majordomo is a "reliable" message broker, you might be tempted to add some spinning rust (that is, ferrous-based hard disk platters). After all, this works for all the enterprise messaging systems. It's such a tempting idea that it's a little sad to have to be negative toward it. But brutal cynicism is one of my specialties. So, some reasons you don't want rust-based brokers sitting in the center of your architecture are:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>As you've seen, the Lazy Pirate client performs surprisingly well. It works across a whole range of architectures, from direct client-to-server to distributed queue proxies. It does tend to assume that workers are stateless and idempotent. But we can work around that limitation without resorting to rust.</p></li><li class="listitem"><p>Rust brings a whole set of problems, from slow performance to additional pieces that you have to manage, repair, and handle 6 a.m. panics from, as they inevitably break at the start of daily operations. The beauty of the Pirate patterns in general is their simplicity. They won't crash. And if you're still worried about the hardware, you can move to a peer-to-peer pattern that has no broker at all. I'll explain later in this chapter.</p></li></ul></div><p>Having said this, however, there is one sane use case for rust-based reliability, which is an asynchronous disconnected network. It solves a major problem with Pirate, namely that a client has to wait for an answer in real time. If clients and workers are only sporadically connected (think of email as an analogy), we can't use a stateless network between clients and workers. We have to put state in the middle.</p><p>So, here's the Titanic pattern<a class="xref" href="ch04s12.html#figure-51" title="Figure 4.5. The Titanic Pattern">Figure 4.5, “The Titanic Pattern”</a>, in which we write messages to disk to ensure they never get lost, no matter how sporadically clients and workers are connected. As we did for service discovery, we're going to layer Titanic on top of MDP rather than extend it. It's wonderfully lazy because it means we can implement our fire-and-forget reliability in a specialized worker, rather than in the broker. This is excellent for several reasons:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>It is <span class="emphasis"><em>much</em></span> easier because we divide and conquer: the broker handles message routing and the worker handles reliability.</p></li><li class="listitem"><p>It lets us mix brokers written in one language with workers written in another.</p></li><li class="listitem"><p>It lets us evolve the fire-and-forget technology independently.</p></li></ul></div><p>The only downside is that there's an extra network hop between broker and hard disk. The benefits are easily worth it.</p><p>There are many ways to make a persistent request-reply architecture. We'll aim for one that is simple and painless. The simplest design I could come up with, after playing with this for a few hours, is a "proxy service". That is, Titanic doesn't affect workers at all. If a client wants a reply immediately, it talks directly to a service and hopes the service is available. If a client is happy to wait a while, it talks to Titanic instead and asks, "hey, buddy, would you take care of this for me while I go buy my groceries?"</p><div class="figure"><a id="figure-51"></a><p class="title"><strong>Figure 4.5. The Titanic Pattern</strong></p><div class="figure-contents"><div class="mediaobject"><img src="images/fig51.png" alt="The Titanic Pattern" /></div></div></div><br class="figure-break" /><p>Titanic is thus both a worker and a client. The dialog between client and Titanic goes along these lines:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Client: Please accept this request for me. Titanic: OK, done.</p></li><li class="listitem"><p>Client: Do you have a reply for me? Titanic: Yes, here it is. Or, no, not yet.</p></li><li class="listitem"><p>Client: OK, you can wipe that request now, I'm happy. Titanic: OK, done.</p></li></ul></div><p>Whereas the dialog between Titanic and broker and worker goes like this:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Titanic: Hey, Broker, is there an coffee service? Broker: Uhm, Yeah, seems like.</p></li><li class="listitem"><p>Titanic: Hey, coffee service, please handle this for me.</p></li><li class="listitem"><p>Coffee: Sure, here you are.</p></li><li class="listitem"><p>Titanic: Sweeeeet!</p></li></ul></div><p>You can work through this and the possible failure scenarios. If a worker crashes while processing a request, Titanic retries indefinitely. If a reply gets lost somewhere, Titanic will retry. If the request gets processed but the client doesn't get the reply, it will ask again. If Titanic crashes while processing a request or a reply, the client will try again. As long as requests are fully committed to safe storage, work can't get lost.</p><p>The handshaking is pedantic, but can be pipelined, i.e., clients can use the asynchronous Majordomo pattern to do a lot of work and then get the responses later.</p><p>We need some way for a client to request <span class="emphasis"><em>its</em></span> replies. We'll have many clients asking for the same services, and clients disappear and reappear with different identities. Here is a simple, reasonably secure solution:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Every request generates a universally unique ID (UUID), which Titanic returns to the client after it has queued the request.</p></li><li class="listitem"><p>When a client asks for a reply, it must specify the UUID for the original request.</p></li></ul></div><p>In a realistic case, the client would want to store its request UUIDs safely, e.g., in a local database.</p><p>Before we jump off and write yet another formal specification (fun, fun!), let's consider how the client talks to Titanic. One way is to use a single service and send it three different request types. Another way, which seems simpler, is to use three services:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p><code class="literal">titanic.request</code>: store a request message, and return a UUID for the request.</p></li><li class="listitem"><p><code class="literal">titanic.reply</code>: fetch a reply, if available, for a given request UUID.</p></li><li class="listitem"><p><code class="literal">titanic.close</code>: confirm that a reply has been stored and processed.</p></li></ul></div><p>We'll just make a multithreaded worker, which as we've seen from our multithreading experience with ØMQ, is trivial. However, let's first sketch what Titanic would look like in terms of ØMQ messages and frames. This gives us the <a class="ulink" href="http://rfc.zeromq.org/spec:9" target="_top">Titanic Service Protocol (TSP)</a>.</p><p>Using TSP is clearly more work for client applications than accessing a service directly via MDP. Here's the shortest robust "echo" client example:</p><div class="example"><a id="ticlient-c"></a><p class="title"><strong>Example 4.53. Titanic client example (ticlient.c)</strong></p><div class="example-contents"><pre class="programlisting">
//  Titanic client example
//  Implements client side of http://rfc.zeromq.org/spec:9

//  Lets build this source without creating a library
#include "mdcliapi.c"

//  Calls a TSP service
//  Returns response if successful (status code 200 OK), else NULL
//
static zmsg_t *
s_service_call (mdcli_t *session, char *service, zmsg_t **request_p)
{
    zmsg_t *reply = mdcli_send (session, service, request_p);
    if (reply) {
        zframe_t *status = zmsg_pop (reply);
        if (zframe_streq (status, "200")) {
            zframe_destroy (&amp;status);
            return reply;
        }
        else
        if (zframe_streq (status, "400")) {
            printf ("E: client fatal error, aborting\n");
            exit (EXIT_FAILURE);
        }
        else
        if (zframe_streq (status, "500")) {
            printf ("E: server fatal error, aborting\n");
            exit (EXIT_FAILURE);
        }
    }
    else
        exit (EXIT_SUCCESS);    //  Interrupted or failed

    zmsg_destroy (&amp;reply);
    return NULL;        //  Didn't succeed; don't care why not
}
</pre></div></div><br class="example-break" /><p>The main task tests our service call by sending an echo request: 
</p><div class="example"><a id="ticlient-c-1"></a><p class="title"><strong>Example 4.54. Titanic client example (ticlient.c) - main task</strong></p><div class="example-contents"><pre class="programlisting">

int main (int argc, char *argv [])
{
    int verbose = (argc &gt; 1 &amp;&amp; streq (argv [1], "-v"));
    mdcli_t *session = mdcli_new ("tcp://localhost:5555", verbose);

    //  1. Send 'echo' request to Titanic
    zmsg_t *request = zmsg_new ();
    zmsg_addstr (request, "echo");
    zmsg_addstr (request, "Hello world");
    zmsg_t *reply = s_service_call (
        session, "titanic.request", &amp;request);

    zframe_t *uuid = NULL;
    if (reply) {
        uuid = zmsg_pop (reply);
        zmsg_destroy (&amp;reply);
        zframe_print (uuid, "I: request UUID ");
    }
    //  2. Wait until we get a reply
    while (!zctx_interrupted) {
        zclock_sleep (100);
        request = zmsg_new ();
        zmsg_add (request, zframe_dup (uuid));
        zmsg_t *reply = s_service_call (
            session, "titanic.reply", &amp;request);

        if (reply) {
            char *reply_string = zframe_strdup (zmsg_last (reply));
            printf ("Reply: %s\n", reply_string);
            free (reply_string);
            zmsg_destroy (&amp;reply);

            //  3. Close request
            request = zmsg_new ();
            zmsg_add (request, zframe_dup (uuid));
            reply = s_service_call (session, "titanic.close", &amp;request);
            zmsg_destroy (&amp;reply);
            break;
        }
        else {
            printf ("I: no reply yet, trying again...\n");
            zclock_sleep (5000);     //  Try again in 5 seconds
        }
    }
    zframe_destroy (&amp;uuid);
    mdcli_destroy (&amp;session);
    return 0;
}
</pre></div></div><br class="example-break" /><p>Of course this can be, and should be, wrapped up in some kind of framework or API. It's not healthy to ask average application developers to learn the full details of messaging: it hurts their brains, costs time, and offers too many ways to make buggy complexity. Additionally, it makes it hard to add intelligence.</p><p>For example, this client blocks on each request whereas in a real application, we'd want to be doing useful work while tasks are executed. This requires some nontrivial plumbing to build a background thread and talk to that cleanly. It's the kind of thing you want to wrap in a nice simple API that the average developer cannot misuse. It's the same approach that we used for Majordomo.</p><p>Here's the Titanic implementation. This server handles the three services using three threads, as proposed. It does full persistence to disk using the most brutal approach possible: one file per message. It's so simple, it's scary. The only complex part is that it keeps a separate queue of all requests, to avoid reading the directory over and over:</p><div class="example"><a id="titanic-c"></a><p class="title"><strong>Example 4.55. Titanic broker example (titanic.c)</strong></p><div class="example-contents"><pre class="programlisting">
//  Titanic service
//  Implements server side of http://rfc.zeromq.org/spec:9

//  Lets us build this source without creating a library
#include "mdwrkapi.c"
#include "mdcliapi.c"

#include "zfile.h"
#include &lt;uuid/uuid.h&gt;

//  Return a new UUID as a printable character string
//  Caller must free returned string when finished with it

static char *
s_generate_uuid (void)
{
    char hex_char [] = "0123456789ABCDEF";
    char *uuidstr = zmalloc (sizeof (uuid_t) * 2 + 1);
    uuid_t uuid;
    uuid_generate (uuid);
    int byte_nbr;
    for (byte_nbr = 0; byte_nbr &lt; sizeof (uuid_t); byte_nbr++) {
        uuidstr [byte_nbr * 2 + 0] = hex_char [uuid [byte_nbr] &gt;&gt; 4];
        uuidstr [byte_nbr * 2 + 1] = hex_char [uuid [byte_nbr] &amp; 15];
    }
    return uuidstr;
}

//  Returns freshly allocated request filename for given UUID

#define TITANIC_DIR ".titanic"

static char *
s_request_filename (char *uuid) {
    char *filename = malloc (256);
    snprintf (filename, 256, TITANIC_DIR "/%s.req", uuid);
    return filename;
}

//  Returns freshly allocated reply filename for given UUID

static char *
s_reply_filename (char *uuid) {
    char *filename = malloc (256);
    snprintf (filename, 256, TITANIC_DIR "/%s.rep", uuid);
    return filename;
}
</pre></div></div><br class="example-break" /><p>The <code class="literal">titanic.request</code> task waits for requests to this service. It writes each request to disk and returns a UUID to the client. The client picks up the reply asynchronously using the <code class="literal">titanic.reply</code> service: 
</p><div class="example"><a id="titanic-c-1"></a><p class="title"><strong>Example 4.56. Titanic broker example (titanic.c) - Titanic request service</strong></p><div class="example-contents"><pre class="programlisting">

static void
titanic_request (void *args, zctx_t *ctx, void *pipe)
{
    mdwrk_t *worker = mdwrk_new (
        "tcp://localhost:5555", "titanic.request", 0);
    zmsg_t *reply = NULL;

    while (true) {
        //  Send reply if it's not null
        //  And then get next request from broker
        zmsg_t *request = mdwrk_recv (worker, &amp;reply);
        if (!request)
            break;      //  Interrupted, exit

        //  Ensure message directory exists
        zfile_mkdir (TITANIC_DIR);

        //  Generate UUID and save message to disk
        char *uuid = s_generate_uuid ();
        char *filename = s_request_filename (uuid);
        FILE *file = fopen (filename, "w");
        assert (file);
        zmsg_save (request, file);
        fclose (file);
        free (filename);
        zmsg_destroy (&amp;request);

        //  Send UUID through to message queue
        reply = zmsg_new ();
        zmsg_addstr (reply, uuid);
        zmsg_send (&amp;reply, pipe);

        //  Now send UUID back to client
        //  Done by the mdwrk_recv() at the top of the loop
        reply = zmsg_new ();
        zmsg_addstr (reply, "200");
        zmsg_addstr (reply, uuid);
        free (uuid);
    }
    mdwrk_destroy (&amp;worker);
}
</pre></div></div><br class="example-break" /><p>The <code class="literal">titanic.reply</code> task checks if there's a reply for the specified request (by UUID), and returns a 200 (OK), 300 (Pending), or 400 (Unknown) accordingly: 
</p><div class="example"><a id="titanic-c-2"></a><p class="title"><strong>Example 4.57. Titanic broker example (titanic.c) - Titanic reply service</strong></p><div class="example-contents"><pre class="programlisting">

static void *
titanic_reply (void *context)
{
    mdwrk_t *worker = mdwrk_new (
        "tcp://localhost:5555", "titanic.reply", 0);
    zmsg_t *reply = NULL;

    while (true) {
        zmsg_t *request = mdwrk_recv (worker, &amp;reply);
        if (!request)
            break;      //  Interrupted, exit

        char *uuid = zmsg_popstr (request);
        char *req_filename = s_request_filename (uuid);
        char *rep_filename = s_reply_filename (uuid);
        if (zfile_exists (rep_filename)) {
            FILE *file = fopen (rep_filename, "r");
            assert (file);
            reply = zmsg_load (NULL, file);
            zmsg_pushstr (reply, "200");
            fclose (file);
        }
        else {
            reply = zmsg_new ();
            if (zfile_exists (req_filename))
                zmsg_pushstr (reply, "300"); //Pending
            else
                zmsg_pushstr (reply, "400"); //Unknown
        }
        zmsg_destroy (&amp;request);
        free (uuid);
        free (req_filename);
        free (rep_filename);
    }
    mdwrk_destroy (&amp;worker);
    return 0;
}
</pre></div></div><br class="example-break" /><p>The <code class="literal">titanic.close</code> task removes any waiting replies for the request (specified by UUID). It's idempotent, so it is safe to call more than once in a row: 
</p><div class="example"><a id="titanic-c-3"></a><p class="title"><strong>Example 4.58. Titanic broker example (titanic.c) - Titanic close task</strong></p><div class="example-contents"><pre class="programlisting">

static void *
titanic_close (void *context)
{
    mdwrk_t *worker = mdwrk_new (
        "tcp://localhost:5555", "titanic.close", 0);
    zmsg_t *reply = NULL;

    while (true) {
        zmsg_t *request = mdwrk_recv (worker, &amp;reply);
        if (!request)
            break;      //  Interrupted, exit

        char *uuid = zmsg_popstr (request);
        char *req_filename = s_request_filename (uuid);
        char *rep_filename = s_reply_filename (uuid);
        zfile_delete (req_filename);
        zfile_delete (rep_filename);
        free (uuid);
        free (req_filename);
        free (rep_filename);

        zmsg_destroy (&amp;request);
        reply = zmsg_new ();
        zmsg_addstr (reply, "200");
    }
    mdwrk_destroy (&amp;worker);
    return 0;
}
</pre></div></div><br class="example-break" /><p>This is the main thread for the Titanic worker. It starts three child threads; for the request, reply, and close services. It then dispatches requests to workers using a simple brute force disk queue. It receives request UUIDs from the <code class="literal">titanic.request</code> service, saves these to a disk file, and then throws each request at MDP workers until it gets a response. 
</p><div class="example"><a id="titanic-c-4"></a><p class="title"><strong>Example 4.59. Titanic broker example (titanic.c) - worker task</strong></p><div class="example-contents"><pre class="programlisting">

static int s_service_success (char *uuid);

int main (int argc, char *argv [])
{
    int verbose = (argc &gt; 1 &amp;&amp; streq (argv [1], "-v"));
    zctx_t *ctx = zctx_new ();

    void *request_pipe = zthread_fork (ctx, titanic_request, NULL);
    zthread_new (titanic_reply, NULL);
    zthread_new (titanic_close, NULL);

    //  Main dispatcher loop
    while (true) {
        //  We'll dispatch once per second, if there's no activity
        zmq_pollitem_t items [] = { { request_pipe, 0, ZMQ_POLLIN, 0 } };
        int rc = zmq_poll (items, 1, 1000 * ZMQ_POLL_MSEC);
        if (rc == -1)
            break;              //  Interrupted
        if (items [0].revents &amp; ZMQ_POLLIN) {
            //  Ensure message directory exists
            zfile_mkdir (TITANIC_DIR);

            //  Append UUID to queue, prefixed with '-' for pending
            zmsg_t *msg = zmsg_recv (request_pipe);
            if (!msg)
                break;          //  Interrupted
            FILE *file = fopen (TITANIC_DIR "/queue", "a");
            char *uuid = zmsg_popstr (msg);
            fprintf (file, "-%s\n", uuid);
            fclose (file);
            free (uuid);
            zmsg_destroy (&amp;msg);
        }
        //  Brute force dispatcher
        char entry [] = "?.......:.......:.......:.......:";
        FILE *file = fopen (TITANIC_DIR "/queue", "r+");
        while (file &amp;&amp; fread (entry, 33, 1, file) == 1) {
            //  UUID is prefixed with '-' if still waiting
            if (entry [0] == '-') {
                if (verbose)
                    printf ("I: processing request %s\n", entry + 1);
                if (s_service_success (entry + 1)) {
                    //  Mark queue entry as processed
                    fseek (file, -33, SEEK_CUR);
                    fwrite ("+", 1, 1, file);
                    fseek (file, 32, SEEK_CUR);
                }
            }
            //  Skip end of line, LF or CRLF
            if (fgetc (file) == '\r')
                fgetc (file);
            if (zctx_interrupted)
                break;
        }
        if (file)
            fclose (file);
    }
    return 0;
}
</pre></div></div><br class="example-break" /><p>Here, we first check if the requested MDP service is defined or not, using a MMI lookup to the Majordomo broker. If the service exists, we send a request and wait for a reply using the conventional MDP client API. This is not meant to be fast, just very simple: 
</p><div class="example"><a id="titanic-c-5"></a><p class="title"><strong>Example 4.60. Titanic broker example (titanic.c) - try to call a service</strong></p><div class="example-contents"><pre class="programlisting">

static int
s_service_success (char *uuid)
{
    //  Load request message, service will be first frame
    char *filename = s_request_filename (uuid);
    FILE *file = fopen (filename, "r");
    free (filename);

    //  If the client already closed request, treat as successful
    if (!file)
        return 1;

    zmsg_t *request = zmsg_load (NULL, file);
    fclose (file);
    zframe_t *service = zmsg_pop (request);
    char *service_name = zframe_strdup (service);

    //  Create MDP client session with short timeout
    mdcli_t *client = mdcli_new ("tcp://localhost:5555", false);
    mdcli_set_timeout (client, 1000);  //  1 sec
    mdcli_set_retries (client, 1);     //  only 1 retry

    //  Use MMI protocol to check if service is available
    zmsg_t *mmi_request = zmsg_new ();
    zmsg_add (mmi_request, service);
    zmsg_t *mmi_reply = mdcli_send (client, "mmi.service", &amp;mmi_request);
    int service_ok = (mmi_reply
        &amp;&amp; zframe_streq (zmsg_first (mmi_reply), "200"));
    zmsg_destroy (&amp;mmi_reply);

    int result = 0;
    if (service_ok) {
        zmsg_t *reply = mdcli_send (client, service_name, &amp;request);
        if (reply) {
            filename = s_reply_filename (uuid);
            FILE *file = fopen (filename, "w");
            assert (file);
            zmsg_save (reply, file);
            fclose (file);
            free (filename);
            result = 1;
        }
        zmsg_destroy (&amp;reply);
    }
    else
        zmsg_destroy (&amp;request);

    mdcli_destroy (&amp;client);
    free (service_name);
    return result;
}
</pre></div></div><br class="example-break" /><p>To test this, start <code class="literal">mdbroker</code> and <code class="literal">titanic</code>, and then run <code class="literal">ticlient</code>. Now start <code class="literal">mdworker</code> arbitrarily, and you should see the client getting a response and exiting happily.</p><p>Some notes about this code:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Note that some loops start by sending, others by receiving messages. This is because Titanic acts both as a client and a worker in different roles.</p></li><li class="listitem"><p>The Titanic broker uses the MMI service discovery protocol to send requests only to services that appear to be running. Since the MMI implementation in our little Majordomo broker is quite poor, this won't work all the time.</p></li><li class="listitem"><p>We use an inproc connection to send new request data from the <code class="literal">titanic.request</code> service through to the main dispatcher. This saves the dispatcher from having to scan the disk directory, load all request files, and sort them by date/time.</p></li></ul></div><p>The important thing about this example is not performance (which, although I haven't tested it, is surely terrible), but how well it implements the reliability contract. To try it, start the mdbroker and titanic programs. Then start the ticlient, and then start the mdworker echo service. You can run all four of these using the <code class="literal">-v</code> option to do verbose activity tracing. You can stop and restart any piece <span class="emphasis"><em>except the client</em></span> and nothing will get lost.</p><p>If you want to use Titanic in real cases, you'll rapidly be asking "how do we make this faster?"</p><p>Here's what I'd do, starting with the example implementation:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Use a single disk file for all data, rather than multiple files. Operating systems are usually better at handling a few large files than many smaller ones.</p></li><li class="listitem"><p>Organize that disk file as a circular buffer so that new requests can be written contiguously (with very occasional wraparound). One thread, writing full speed to a disk file, can work rapidly.</p></li><li class="listitem"><p>Keep the index in memory and rebuild the index at startup time, from the disk buffer. This saves the extra disk head flutter needed to keep the index fully safe on disk. You would want an fsync after every message, or every N milliseconds if you were prepared to lose the last M messages in case of a system failure.</p></li><li class="listitem"><p>Use a solid-state drive rather than spinning iron oxide platters.</p></li><li class="listitem"><p>Pre-allocate the entire file, or allocate it in large chunks, which allows the circular buffer to grow and shrink as needed. This avoids fragmentation and ensures that most reads and writes are contiguous.</p></li></ul></div><p>And so on. What I'd not recommend is storing messages in a database, not even a "fast" key/value store, unless you really like a specific database and don't have performance worries. You will pay a steep price for the abstraction, ten to a thousand times over a raw disk file.</p><p>If you want to make Titanic <span class="emphasis"><em>even more reliable</em></span>, duplicate the requests to a second server, which you'd place in a second location just far away enough to survive a nuclear attack on your primary location, yet not so far that you get too much latency.</p><p>If you want to make Titanic <span class="emphasis"><em>much faster and less reliable</em></span>, store requests and replies purely in memory. This will give you the functionality of a disconnected network, but requests won't survive a crash of the Titanic server itself.</p></div><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="ch04s11.html">Prev</a> </td><td width="20%" align="center"><a accesskey="u" href="ch04.html">Up</a></td><td width="40%" align="right"> <a accesskey="n" href="ch04s13.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">Idempotent Services </td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top"> High-Availability Pair (Binary Star Pattern)</td></tr></table></div></body></html>
